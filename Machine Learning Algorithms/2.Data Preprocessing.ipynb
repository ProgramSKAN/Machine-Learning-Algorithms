{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2.Data Preprocessing.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOqNr7g8tH+E1rzc7sRl20K"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"IkQP6SVOIJ1B","colab_type":"text"},"source":["# **Data Preprocessing Tools**"]},{"cell_type":"code","metadata":{"id":"5rClSbfhH8f0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":208},"executionInfo":{"status":"ok","timestamp":1594219519671,"user_tz":-330,"elapsed":652,"user":{"displayName":"Chintu Chintu","photoUrl":"","userId":"16549076222200533733"}},"outputId":"bcd4fb87-756a-4e62-db2d-5270506972ec"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","# Import Data of retail company which contains country,age,salary,isPurchasedProduct?\n","dataset=pd.read_csv('https://raw.githubusercontent.com/ProgramSKAN/Sample-Data-For-Machine-Learning/master/Data.csv')\n","X=dataset.iloc[:, :-1].values #get all rows and all columns except last column\n","y=dataset.iloc[:,-1].values # get only last column\n","print(dataset)"],"execution_count":48,"outputs":[{"output_type":"stream","text":["   Country   Age   Salary Purchased\n","0   France  44.0  72000.0        No\n","1    Spain  27.0  48000.0       Yes\n","2  Germany  30.0  54000.0        No\n","3    Spain  38.0  61000.0        No\n","4  Germany  40.0      NaN       Yes\n","5   France  35.0  58000.0       Yes\n","6    Spain   NaN  52000.0        No\n","7   France  48.0  79000.0       Yes\n","8  Germany  50.0  83000.0        No\n","9   France  37.0  67000.0       Yes\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sEQWzi1DbooI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":191},"executionInfo":{"status":"ok","timestamp":1594219520916,"user_tz":-330,"elapsed":532,"user":{"displayName":"Chintu Chintu","photoUrl":"","userId":"16549076222200533733"}},"outputId":"83a0bf06-5777-4f55-c511-b3433ec2aa9f"},"source":["print(X) #Independent Variable"],"execution_count":49,"outputs":[{"output_type":"stream","text":["[['France' 44.0 72000.0]\n"," ['Spain' 27.0 48000.0]\n"," ['Germany' 30.0 54000.0]\n"," ['Spain' 38.0 61000.0]\n"," ['Germany' 40.0 nan]\n"," ['France' 35.0 58000.0]\n"," ['Spain' nan 52000.0]\n"," ['France' 48.0 79000.0]\n"," ['Germany' 50.0 83000.0]\n"," ['France' 37.0 67000.0]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uLtpOD3jDUAf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1594219523898,"user_tz":-330,"elapsed":1948,"user":{"displayName":"Chintu Chintu","photoUrl":"","userId":"16549076222200533733"}},"outputId":"d05947e4-91a0-4701-aaaa-98d672c2da73"},"source":["print(y) #dependent variable"],"execution_count":50,"outputs":[{"output_type":"stream","text":["['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yza1MWhhLKpU","colab_type":"text"},"source":["Take care of missing values"]},{"cell_type":"code","metadata":{"id":"R8B6mHzKGabL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1594219527717,"user_tz":-330,"elapsed":1589,"user":{"displayName":"Chintu Chintu","photoUrl":"","userId":"16549076222200533733"}},"outputId":"4e002345-eec4-47c3-cbf9-a6b5b021bb11"},"source":["print(np.nan)"],"execution_count":51,"outputs":[{"output_type":"stream","text":["nan\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"W6x-Rv6ZGCfF","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594219530877,"user_tz":-330,"elapsed":1477,"user":{"displayName":"Chintu Chintu","photoUrl":"","userId":"16549076222200533733"}}},"source":["from sklearn.impute import SimpleImputer\n","imputer=SimpleImputer(missing_values=np.nan,strategy='mean')\n","imputer.fit(X[:,1:3])\n","X[:,1:3]=imputer.transform(X[:,1:3])"],"execution_count":52,"outputs":[]},{"cell_type":"code","metadata":{"id":"okPqClXCG_r6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":191},"executionInfo":{"status":"ok","timestamp":1594219532438,"user_tz":-330,"elapsed":589,"user":{"displayName":"Chintu Chintu","photoUrl":"","userId":"16549076222200533733"}},"outputId":"7babc222-32a6-4b29-9ba9-4effbbb79b9f"},"source":["print(X)"],"execution_count":53,"outputs":[{"output_type":"stream","text":["[['France' 44.0 72000.0]\n"," ['Spain' 27.0 48000.0]\n"," ['Germany' 30.0 54000.0]\n"," ['Spain' 38.0 61000.0]\n"," ['Germany' 40.0 63777.77777777778]\n"," ['France' 35.0 58000.0]\n"," ['Spain' 38.77777777777778 52000.0]\n"," ['France' 48.0 79000.0]\n"," ['Germany' 50.0 83000.0]\n"," ['France' 37.0 67000.0]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jFQ281-9IpKw","colab_type":"text"},"source":["### Encoding Categorical Data"]},{"cell_type":"markdown","metadata":{"id":"8MVtxv8DIuM8","colab_type":"text"},"source":["Encoding the Independent variable"]},{"cell_type":"code","metadata":{"id":"JvWNOY6yI68z","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594219535516,"user_tz":-330,"elapsed":722,"user":{"displayName":"Chintu Chintu","photoUrl":"","userId":"16549076222200533733"}}},"source":["from sklearn.compose import ColumnTransformer\n","from sklearn.preprocessing import OneHotEncoder\n","ct=ColumnTransformer(transformers=[('encoder',OneHotEncoder(),[0])],remainder='passthrough')\n","X=np.array(ct.fit_transform(X))"],"execution_count":54,"outputs":[]},{"cell_type":"code","metadata":{"id":"V9ECxClUKguN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":191},"executionInfo":{"status":"ok","timestamp":1594219538470,"user_tz":-330,"elapsed":2345,"user":{"displayName":"Chintu Chintu","photoUrl":"","userId":"16549076222200533733"}},"outputId":"d02911c5-5041-4fe2-de1f-151840c6d44a"},"source":["print(X)"],"execution_count":55,"outputs":[{"output_type":"stream","text":["[[1.0 0.0 0.0 44.0 72000.0]\n"," [0.0 0.0 1.0 27.0 48000.0]\n"," [0.0 1.0 0.0 30.0 54000.0]\n"," [0.0 0.0 1.0 38.0 61000.0]\n"," [0.0 1.0 0.0 40.0 63777.77777777778]\n"," [1.0 0.0 0.0 35.0 58000.0]\n"," [0.0 0.0 1.0 38.77777777777778 52000.0]\n"," [1.0 0.0 0.0 48.0 79000.0]\n"," [0.0 1.0 0.0 50.0 83000.0]\n"," [1.0 0.0 0.0 37.0 67000.0]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lICOMC0oI3jY","colab_type":"text"},"source":["Encoding the Dependent variable"]},{"cell_type":"code","metadata":{"id":"KHnyqCW7Hy1_","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594219541452,"user_tz":-330,"elapsed":719,"user":{"displayName":"Chintu Chintu","photoUrl":"","userId":"16549076222200533733"}}},"source":["from sklearn.preprocessing import LabelEncoder\n","le=LabelEncoder()\n","y=le.fit_transform(y)"],"execution_count":56,"outputs":[]},{"cell_type":"code","metadata":{"id":"_CVaqSoNMCKT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1594219541872,"user_tz":-330,"elapsed":477,"user":{"displayName":"Chintu Chintu","photoUrl":"","userId":"16549076222200533733"}},"outputId":"0e6b4523-7b0d-49e6-da2a-fff82c362a6c"},"source":["print(y)"],"execution_count":57,"outputs":[{"output_type":"stream","text":["[0 1 0 0 1 1 0 1 0 1]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1-GEYxyipMal","colab_type":"text"},"source":["### Spliting the dataset into Train and Test set"]},{"cell_type":"code","metadata":{"id":"BdT8LCzipVjH","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594219544419,"user_tz":-330,"elapsed":886,"user":{"displayName":"Chintu Chintu","photoUrl":"","userId":"16549076222200533733"}}},"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)  #80% train 20% test set, splitted uniform randomly"],"execution_count":58,"outputs":[]},{"cell_type":"code","metadata":{"id":"jnOkwpVit8qR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":156},"executionInfo":{"status":"ok","timestamp":1594219551181,"user_tz":-330,"elapsed":672,"user":{"displayName":"Chintu Chintu","photoUrl":"","userId":"16549076222200533733"}},"outputId":"0f56a8fd-753d-4416-deab-ac108266a163"},"source":["print(X_train)"],"execution_count":59,"outputs":[{"output_type":"stream","text":["[[0.0 0.0 1.0 38.77777777777778 52000.0]\n"," [0.0 1.0 0.0 40.0 63777.77777777778]\n"," [1.0 0.0 0.0 44.0 72000.0]\n"," [0.0 0.0 1.0 38.0 61000.0]\n"," [0.0 0.0 1.0 27.0 48000.0]\n"," [1.0 0.0 0.0 48.0 79000.0]\n"," [0.0 1.0 0.0 50.0 83000.0]\n"," [1.0 0.0 0.0 35.0 58000.0]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gGqKL_vPt_t3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1594219552389,"user_tz":-330,"elapsed":402,"user":{"displayName":"Chintu Chintu","photoUrl":"","userId":"16549076222200533733"}},"outputId":"b5494cb8-6823-4447-867f-db9149c083b0"},"source":["print(X_test)"],"execution_count":60,"outputs":[{"output_type":"stream","text":["[[0.0 1.0 0.0 30.0 54000.0]\n"," [1.0 0.0 0.0 37.0 67000.0]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7z0EsDExuCD9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1594219554270,"user_tz":-330,"elapsed":1344,"user":{"displayName":"Chintu Chintu","photoUrl":"","userId":"16549076222200533733"}},"outputId":"eeb7a2e4-d143-4c2f-97fa-89ef7cfc7756"},"source":["print(y_train)"],"execution_count":61,"outputs":[{"output_type":"stream","text":["[0 1 0 0 1 1 0 1]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"h0nHYGD5uFqv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1594219556706,"user_tz":-330,"elapsed":919,"user":{"displayName":"Chintu Chintu","photoUrl":"","userId":"16549076222200533733"}},"outputId":"f63bb1ec-390d-4759-978e-da83fa118147"},"source":["print(y_test)"],"execution_count":62,"outputs":[{"output_type":"stream","text":["[0 1]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cO3fmYr2pfZU","colab_type":"text"},"source":["### Feature Scaling\n","for scaling all our variables/features to make sure that they all take values in the same scale.This is to prevent one feature to dominate the other which therefore dominated features would be neglected by ML model.Because since most ML model depends on Eclidean distance ie.\n","\n","Eclidean distance between points P1 and P2=sqrt((x2-x1)^2+(y2-y1)^2)\n","so, if we plot the graph between salary and age, then Eclidean distance of salary will dominate by its number.so we need it to be in same scale.\n","\n","Feature scaling should be applied after splitting dataset because the test set supposed to be a brand new set on which we evaluate ML model.since Feature scaling is technique to get a mean and std dev of the feature inorder to perform scaling.so Feature scaling should not be performed on test set.\n","\n","if we apply Feature Scaling before split then it actually get mean and std dev of all the values including the one in test set.since test set is the one we not supposed to have like future data in production.so,applying Feature Scaling before split causes information leakage on test set which we not suppose to have until training is done.\n","\n","we don't use feature scaling for all the models even though we have the features taking very different values.Ex: in multiple linear regression equation (y=b0 + b1*x1 + ....+bn*xn), each variable xn is multiplied by the a coefficeint bn, if some variables that take much higer values than others, when learning the coefficients the coefficients will just compensate by taking small values for the variables that take high values.\n","\n","\n","std dev=sqrt(variance)\n","\n","**Standardisation:**Makes all features to be between -3 and +3.standardization actually works well all the time.\n","\n","Xstand=(x-mean(x))/std dev(x)   \n","\n","\n","**Normalisation:** makes all the values of features to be between 0 and 1.Normalization is recommended when you have a normal distribution in most of your features.\n","\n","Xnorm=(x-min(x))/(max(x)-min(x))\n","\n","*feature scaling should not be applied on test set\n","*do we have to apply feature scaling ( standardization) to the dummy variables in column1 (binary numbers) in the matrix of features.? No, because we loose information of country column.sometime it increases performance of model but still not recommended."]},{"cell_type":"code","metadata":{"id":"Qxh7IiyNrVoq","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594219565308,"user_tz":-330,"elapsed":1755,"user":{"displayName":"Chintu Chintu","photoUrl":"","userId":"16549076222200533733"}}},"source":["from sklearn.preprocessing import StandardScaler\n","sc=StandardScaler()\n","X_train[:,3:]=sc.fit_transform(X_train[:,3:]) #apply it to age and salary not for dummy variable(country).fit only gets mean and std dev.transform applies whole formula\n","X_test[:,3:]=sc.transform(X_test[:,3:]) #apply the same transform for test set that applied on training set"],"execution_count":63,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZQm0hxwL2nZv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":156},"executionInfo":{"status":"ok","timestamp":1594219641505,"user_tz":-330,"elapsed":716,"user":{"displayName":"Chintu Chintu","photoUrl":"","userId":"16549076222200533733"}},"outputId":"657c99e7-5b83-41bf-b42a-d731e2b0a915"},"source":["print(X_train) #now all variables on same scale"],"execution_count":66,"outputs":[{"output_type":"stream","text":["[[0.0 0.0 1.0 -0.19159184384578545 -1.0781259408412425]\n"," [0.0 1.0 0.0 -0.014117293757057777 -0.07013167641635372]\n"," [1.0 0.0 0.0 0.566708506533324 0.633562432710455]\n"," [0.0 0.0 1.0 -0.30453019390224867 -0.30786617274297867]\n"," [0.0 0.0 1.0 -1.9018011447007988 -1.420463615551582]\n"," [1.0 0.0 0.0 1.1475343068237058 1.232653363453549]\n"," [0.0 1.0 0.0 1.4379472069688968 1.5749910381638885]\n"," [1.0 0.0 0.0 -0.7401495441200351 -0.5646194287757332]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"c20dnoqL2sli","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1594219642942,"user_tz":-330,"elapsed":1671,"user":{"displayName":"Chintu Chintu","photoUrl":"","userId":"16549076222200533733"}},"outputId":"cbf0956b-fb18-48b0-f023-fc1da262dffb"},"source":["print(X_test)"],"execution_count":67,"outputs":[{"output_type":"stream","text":["[[0.0 1.0 0.0 -1.4661817944830124 -0.9069571034860727]\n"," [1.0 0.0 0.0 -0.44973664397484414 0.2056403393225306]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1D6NgYFL2vvM","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}