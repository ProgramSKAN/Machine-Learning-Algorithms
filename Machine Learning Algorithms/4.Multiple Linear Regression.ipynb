{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4.Multiple Linear Regression.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOLD5zC9fdUNolD0AG15gZ9"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"J6PPwUmaRyZt","colab_type":"text"},"source":["## **Multiple Linear Regression**\n","\n","y=b0 + b1*x1 + .... + bn*xn\n","\n","here dependent variable depends on multiple independentent variables.\n","\n","\n","### **Assumptions of Linear Regression:**\n","1.Linearity\n","\n","2.Homoscedasticity\n","\n","3.multivariate normality\n","\n","4.Independence of errors\n","\n","5.lack of multicollinearity\n","\n","Before applying Linear regression, check this assumptions are true.\n","\n","**Ex:Dataset>** R&D Spend,Administration,Marketing Spend,State,Profit\n","y=profit\n","x1=R&D Spend\n","x2=Administration\n","x3=Marketing Spend\n","\n","since state is a categorical variable, we need to have dummy variable for this.ie:create a new columns for every category and keep 1/0 in it.ie:california,new york\n","\n","D1=state  >if D1=1 then newyork else california.never include all dummy variable columns.this seems biased since california doesn't have its part in equation since D1=0, but inreality that's not the case because the way regression models work is that they will take by default.ie:the coefficient california is included in b0.\n","\n","so, y=b0 + b1*x1 + b2*x2 + b3*x3 + b4*D1\n","\n","<img\n","\n","\n","never include both dummy variables (california,new york).ie:b4*D1+b5*D2.because it means we are duplicating a variable.since always D2=1-D1 ,the phenomenon where one or several independent variables in a linear regression predict another is called multicollinearity as a result of this effect the model cannot distinguish between the effects of D-1 from the effects from of D2.so it won't work properly.This is called Dummy variable trap.\n","\n","the real problem is that you cannot have these three b0,b4*D1,b5*D2 elements in your model at the same time the constant and both the dummy variables.\n","\n","so,always omit one dummy variable and this applies irrespective of the number of dummy variables they are in that specific dummy set.\n","\n","if there are 2 dummy sets made out of 2 different columns, then apply above for both the dummy sets.\n","\n","### **P-Value:**\n","(for statistial significance)\n","\n","in coin toss\n","H0(Null Hypothesis):Tis is a Fair coin\n","H1(Alternative Hypothesis):This is a not a Fair coin\n","\n","assume H0 is true,based on the experiment see whether we can contradict it.\n","\n","toss1>tails>0.5>50% probability\n","\n","toss2>tails>0.25\n","\n","toss3>tails>0.12\n","\n","toss4>tails>0.06\n","\n","toss5>tails>0.03>ie:if we assume H0 is true then probability of this happenning is only 3%\n","\n","toss6>tails>0.01\n","\n","\n","in above experiment P-Value is droping.after alpha=0.05 we conclude that H0 id not true.ie:we are 5% sure that H0 is true and 95% sure H0 is not true.\n","\n","alpha=confidence level (usually 95%).in medical we set to 99%.\n","<img3\n","\n","\n","### **Building a Model:**\n","\n","If y depends on x1,x2,x3,x4,x5,x6,x7 , we should not take all the independent variables bacause 1.garbagein>garbageout 2.we need to explain why to use specific variable.so use only correct variable which help in predicting y.\n","\n","**construct model:** \n","\n","Five methods of building model:\n","\n","1.All-in:: using all variables beacause we know it is useful or may be intentional.it is also done for preparing for backward eleimination.\n","\n","2.Backward Elimination:: \n","  a)select significance level to stay in the model (Ex: SL=0.05)\n","  b)fit the full model with all possible predictors\n","  c)Consider predictor with highest P-Value.if P>SL, go to step d    otherwise FIN(finish ie:model is ready)\n","  d)Remove the predictor\n","  f)fir the model without this variable.goto step c\n","\n","3.Forward Selection::\n","  a)select significance level to enter the model (Ex: SL=0.05)\n","  b)fit all the simple regression models y~xn.select one with lowest p-value.\n","  c)Keep this variable and fit all possible models with one extra predictor added to the one's you already have.\n","  d)consider the predictor with lowest P-value.if P<SL, go to step c   otherwise FIN(finish ie:Keep the previous model)\n","\n","\n","4.Bidirectional Elimination::\n","  a)select significance level to enter and to stay in the model (Ex: SLENTER=0.05,SLSTAY=0.05)\n","  b)perform the next step of forward selection (new variables must have P<SLENTER to enter)\n","  c)perform all the steps of backward elimination (old variables must have P<SLSTAY to stay).goto step b\n","  d)no variables can enter  and no old variables can exit.so FIN(model is ready)\n","\n","5.All Possible models(Score Comparison)::\n","  a)select a criterion of goodness of fit(Ex:Akaike criterion)\n","  b)construct all possible regression models. (2^N) - 1 total combinations.\n","  Ex:if we have 10 columns in dataset,then its (2^10)-1=1023 models\n","  c)select the one with best criterion\n","  d)FINISH(model is ready)\n","\n","2,3,4 refers to Stepwise Regression"]},{"cell_type":"code","metadata":{"id":"ixGrH5dNRtZk","colab_type":"code","colab":{}},"source":["#given dataset from of R&D Spend,Administration,Marketing Spend,State,Profit, predict which is viable startup to invest fund?\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"97BZqIr4X6WY","colab_type":"text"},"source":["### Import Dataset"]},{"cell_type":"code","metadata":{"id":"uwwSeaENX5pP","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594297007719,"user_tz":-330,"elapsed":897,"user":{"displayName":"Chintu Chintu","photoUrl":"","userId":"16549076222200533733"}}},"source":["dataset=pd.read_csv('https://raw.githubusercontent.com/ProgramSKAN/Sample-Data-For-Machine-Learning/master/50_Startups.csv')\n","X=dataset.iloc[:,:-1].values\n","y=dataset.iloc[:,-1].values"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"id":"h2VN-xpYdm3s","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":884},"executionInfo":{"status":"ok","timestamp":1594297008930,"user_tz":-330,"elapsed":1818,"user":{"displayName":"Chintu Chintu","photoUrl":"","userId":"16549076222200533733"}},"outputId":"7ab018cb-721c-4aef-9d56-5b2f77caf0e6"},"source":["print(X)"],"execution_count":32,"outputs":[{"output_type":"stream","text":["[[165349.2 136897.8 471784.1 'New York']\n"," [162597.7 151377.59 443898.53 'California']\n"," [153441.51 101145.55 407934.54 'Florida']\n"," [144372.41 118671.85 383199.62 'New York']\n"," [142107.34 91391.77 366168.42 'Florida']\n"," [131876.9 99814.71 362861.36 'New York']\n"," [134615.46 147198.87 127716.82 'California']\n"," [130298.13 145530.06 323876.68 'Florida']\n"," [120542.52 148718.95 311613.29 'New York']\n"," [123334.88 108679.17 304981.62 'California']\n"," [101913.08 110594.11 229160.95 'Florida']\n"," [100671.96 91790.61 249744.55 'California']\n"," [93863.75 127320.38 249839.44 'Florida']\n"," [91992.39 135495.07 252664.93 'California']\n"," [119943.24 156547.42 256512.92 'Florida']\n"," [114523.61 122616.84 261776.23 'New York']\n"," [78013.11 121597.55 264346.06 'California']\n"," [94657.16 145077.58 282574.31 'New York']\n"," [91749.16 114175.79 294919.57 'Florida']\n"," [86419.7 153514.11 0.0 'New York']\n"," [76253.86 113867.3 298664.47 'California']\n"," [78389.47 153773.43 299737.29 'New York']\n"," [73994.56 122782.75 303319.26 'Florida']\n"," [67532.53 105751.03 304768.73 'Florida']\n"," [77044.01 99281.34 140574.81 'New York']\n"," [64664.71 139553.16 137962.62 'California']\n"," [75328.87 144135.98 134050.07 'Florida']\n"," [72107.6 127864.55 353183.81 'New York']\n"," [66051.52 182645.56 118148.2 'Florida']\n"," [65605.48 153032.06 107138.38 'New York']\n"," [61994.48 115641.28 91131.24 'Florida']\n"," [61136.38 152701.92 88218.23 'New York']\n"," [63408.86 129219.61 46085.25 'California']\n"," [55493.95 103057.49 214634.81 'Florida']\n"," [46426.07 157693.92 210797.67 'California']\n"," [46014.02 85047.44 205517.64 'New York']\n"," [28663.76 127056.21 201126.82 'Florida']\n"," [44069.95 51283.14 197029.42 'California']\n"," [20229.59 65947.93 185265.1 'New York']\n"," [38558.51 82982.09 174999.3 'California']\n"," [28754.33 118546.05 172795.67 'California']\n"," [27892.92 84710.77 164470.71 'Florida']\n"," [23640.93 96189.63 148001.11 'California']\n"," [15505.73 127382.3 35534.17 'New York']\n"," [22177.74 154806.14 28334.72 'California']\n"," [1000.23 124153.04 1903.93 'New York']\n"," [1315.46 115816.21 297114.46 'Florida']\n"," [0.0 135426.92 0.0 'California']\n"," [542.05 51743.15 0.0 'New York']\n"," [0.0 116983.8 45173.06 'California']]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DSZuxiafYXaH","colab_type":"text"},"source":["### Encoding categorical data"]},{"cell_type":"code","metadata":{"id":"0PFrgs31YW2Z","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594297009483,"user_tz":-330,"elapsed":1776,"user":{"displayName":"Chintu Chintu","photoUrl":"","userId":"16549076222200533733"}}},"source":["from sklearn.compose import ColumnTransformer\n","from sklearn.preprocessing import OneHotEncoder\n","ct=ColumnTransformer(transformers=[('encoder',OneHotEncoder(),[3])],remainder='passthrough') # apply onehotencoder to state(categorical) column.the encoded data will be at the beginning\n","X=np.array(ct.fit_transform(X))"],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"id":"kRjZbDWueVKw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":884},"executionInfo":{"status":"ok","timestamp":1594297009483,"user_tz":-330,"elapsed":1411,"user":{"displayName":"Chintu Chintu","photoUrl":"","userId":"16549076222200533733"}},"outputId":"ebd4bd64-df96-4234-a19c-b5bef22d7023"},"source":["print(X)"],"execution_count":34,"outputs":[{"output_type":"stream","text":["[[0.0 0.0 1.0 165349.2 136897.8 471784.1]\n"," [1.0 0.0 0.0 162597.7 151377.59 443898.53]\n"," [0.0 1.0 0.0 153441.51 101145.55 407934.54]\n"," [0.0 0.0 1.0 144372.41 118671.85 383199.62]\n"," [0.0 1.0 0.0 142107.34 91391.77 366168.42]\n"," [0.0 0.0 1.0 131876.9 99814.71 362861.36]\n"," [1.0 0.0 0.0 134615.46 147198.87 127716.82]\n"," [0.0 1.0 0.0 130298.13 145530.06 323876.68]\n"," [0.0 0.0 1.0 120542.52 148718.95 311613.29]\n"," [1.0 0.0 0.0 123334.88 108679.17 304981.62]\n"," [0.0 1.0 0.0 101913.08 110594.11 229160.95]\n"," [1.0 0.0 0.0 100671.96 91790.61 249744.55]\n"," [0.0 1.0 0.0 93863.75 127320.38 249839.44]\n"," [1.0 0.0 0.0 91992.39 135495.07 252664.93]\n"," [0.0 1.0 0.0 119943.24 156547.42 256512.92]\n"," [0.0 0.0 1.0 114523.61 122616.84 261776.23]\n"," [1.0 0.0 0.0 78013.11 121597.55 264346.06]\n"," [0.0 0.0 1.0 94657.16 145077.58 282574.31]\n"," [0.0 1.0 0.0 91749.16 114175.79 294919.57]\n"," [0.0 0.0 1.0 86419.7 153514.11 0.0]\n"," [1.0 0.0 0.0 76253.86 113867.3 298664.47]\n"," [0.0 0.0 1.0 78389.47 153773.43 299737.29]\n"," [0.0 1.0 0.0 73994.56 122782.75 303319.26]\n"," [0.0 1.0 0.0 67532.53 105751.03 304768.73]\n"," [0.0 0.0 1.0 77044.01 99281.34 140574.81]\n"," [1.0 0.0 0.0 64664.71 139553.16 137962.62]\n"," [0.0 1.0 0.0 75328.87 144135.98 134050.07]\n"," [0.0 0.0 1.0 72107.6 127864.55 353183.81]\n"," [0.0 1.0 0.0 66051.52 182645.56 118148.2]\n"," [0.0 0.0 1.0 65605.48 153032.06 107138.38]\n"," [0.0 1.0 0.0 61994.48 115641.28 91131.24]\n"," [0.0 0.0 1.0 61136.38 152701.92 88218.23]\n"," [1.0 0.0 0.0 63408.86 129219.61 46085.25]\n"," [0.0 1.0 0.0 55493.95 103057.49 214634.81]\n"," [1.0 0.0 0.0 46426.07 157693.92 210797.67]\n"," [0.0 0.0 1.0 46014.02 85047.44 205517.64]\n"," [0.0 1.0 0.0 28663.76 127056.21 201126.82]\n"," [1.0 0.0 0.0 44069.95 51283.14 197029.42]\n"," [0.0 0.0 1.0 20229.59 65947.93 185265.1]\n"," [1.0 0.0 0.0 38558.51 82982.09 174999.3]\n"," [1.0 0.0 0.0 28754.33 118546.05 172795.67]\n"," [0.0 1.0 0.0 27892.92 84710.77 164470.71]\n"," [1.0 0.0 0.0 23640.93 96189.63 148001.11]\n"," [0.0 0.0 1.0 15505.73 127382.3 35534.17]\n"," [1.0 0.0 0.0 22177.74 154806.14 28334.72]\n"," [0.0 0.0 1.0 1000.23 124153.04 1903.93]\n"," [0.0 1.0 0.0 1315.46 115816.21 297114.46]\n"," [1.0 0.0 0.0 0.0 135426.92 0.0]\n"," [0.0 0.0 1.0 542.05 51743.15 0.0]\n"," [1.0 0.0 0.0 0.0 116983.8 45173.06]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TU4vOsVckyEp","colab_type":"text"},"source":["### Splitting Dataset into Train and Test Set"]},{"cell_type":"code","metadata":{"id":"CmnTNFTjk3Ue","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594298306539,"user_tz":-330,"elapsed":841,"user":{"displayName":"Chintu Chintu","photoUrl":"","userId":"16549076222200533733"}}},"source":["from sklearn.model_selection import train_test_split\n","X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)"],"execution_count":37,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ig6fD0qGhA0S","colab_type":"text"},"source":["**Feature Scaling** is not required in multiple linear regression because in the equation of the multiple linear regression we have coefficients that is multiplied to each independent viable of each feature and therefore it doesn't matter that some features have higher values than others because the coefficients will compensate to put everything on the same scale."]},{"cell_type":"markdown","metadata":{"id":"tPmvQijChyWR","colab_type":"text"},"source":["We dont need to check all the assumptions of multiple linear reggression in the interest of time.Instead we check multiple models whether it is giving better result or not"]},{"cell_type":"markdown","metadata":{"id":"KhOpEN2yigo1","colab_type":"text"},"source":["### Training Multiple Linear Regression on the Training Set"]},{"cell_type":"code","metadata":{"id":"ybmp1FvRhAdU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1594298309162,"user_tz":-330,"elapsed":991,"user":{"displayName":"Chintu Chintu","photoUrl":"","userId":"16549076222200533733"}},"outputId":"09222fa3-aaed-46de-dfce-50055047956f"},"source":["from sklearn.linear_model import LinearRegression#dummy variable trap will indeed avoided by this class itself\n","#backward elimination technique is also done by the class.this class will  automatically identify the best features of the features that have the highest p values or that are the most statistically significant to figure out how to predict.\n","\n","regressor=LinearRegression()\n","regressor.fit(X_train,y_train)"],"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"markdown","metadata":{"id":"DeXUZeruljnF","colab_type":"text"},"source":["### Predicting Test Set Results"]},{"cell_type":"code","metadata":{"id":"NC9CMDDaee7R","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":191},"executionInfo":{"status":"ok","timestamp":1594299159521,"user_tz":-330,"elapsed":1015,"user":{"displayName":"Chintu Chintu","photoUrl":"","userId":"16549076222200533733"}},"outputId":"45f814d9-30db-4417-a91a-f39c36649d37"},"source":["#not plotting since 4 features,so we need 5D\n","y_pred=regressor.predict(X_test)\n","np.set_printoptions(precision=2) #this will display any numerical value with only 2 decimals after ,\n","print(np.concatenate((y_pred.reshape(len(y_pred),1),y_test.reshape(len(y_test),1)),axis=1)) #axis=0 vertical concatination,axis=1 horizantal concatination\n","#[predicted profit,actual profit]"],"execution_count":42,"outputs":[{"output_type":"stream","text":["[[103015.2  103282.38]\n"," [132582.28 144259.4 ]\n"," [132447.74 146121.95]\n"," [ 71976.1   77798.83]\n"," [178537.48 191050.39]\n"," [116161.24 105008.31]\n"," [ 67851.69  81229.06]\n"," [ 98791.73  97483.56]\n"," [113969.44 110352.25]\n"," [167921.07 166187.94]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5p136WrJFBLj","colab_type":"text"},"source":["Making a single prediction (for example the profit of a startup with R&D Spend = 160000, Administration Spend = 130000, Marketing Spend = 300000 and State = 'California')"]},{"cell_type":"code","metadata":{"id":"d9wwLtdJqeho","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1594299665646,"user_tz":-330,"elapsed":1048,"user":{"displayName":"Chintu Chintu","photoUrl":"","userId":"16549076222200533733"}},"outputId":"821df14a-a02c-4ed9-855a-5fe5c556f2c4"},"source":["print(regressor.predict([[1, 0, 0, 160000, 130000, 300000]]))"],"execution_count":45,"outputs":[{"output_type":"stream","text":["[181566.92]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TXf9NlRFGlht","colab_type":"text"},"source":["Therefore, our model predicts that the profit of a Californian startup which spent 160000 in R&D, 130000 in Administration and 300000 in Marketing is $ 181566,92.\n","\n","**Important note 1:** Notice that the values of the features were all input in a double pair of square brackets. That's because the \"predict\" method always expects a 2D array as the format of its inputs. And putting our values into a double pair of square brackets makes the input exactly a 2D array. Simply put:\n","\n","$1, 0, 0, 160000, 130000, 300000 \\rightarrow \\textrm{scalars}$\n","\n","$[1, 0, 0, 160000, 130000, 300000] \\rightarrow \\textrm{1D array}$\n","\n","$[[1, 0, 0, 160000, 130000, 300000]] \\rightarrow \\textrm{2D array}$\n","\n","**Important note 2:** Notice also that the \"California\" state was not input as a string in the last column but as \"1, 0, 0\" in the first three columns. That's because of course the predict method expects the one-hot-encoded values of the state, and as we see in the second row of the matrix of features X, \"California\" was encoded as \"1, 0, 0\". And be careful to include these values in the first three columns, not the last three ones, because the dummy variables are always created in the first columns."]},{"cell_type":"markdown","metadata":{"id":"soirA4ugKRTL","colab_type":"text"},"source":["### Getting the final linear regression equation with the values of the coefficients"]},{"cell_type":"code","metadata":{"id":"0oQi7yNrq7t5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1594299764343,"user_tz":-330,"elapsed":934,"user":{"displayName":"Chintu Chintu","photoUrl":"","userId":"16549076222200533733"}},"outputId":"da819191-1275-4431-8079-da8ccb40e1e7"},"source":["print(regressor.coef_)\n","print(regressor.intercept_)"],"execution_count":46,"outputs":[{"output_type":"stream","text":["[ 8.66e+01 -8.73e+02  7.86e+02  7.73e-01  3.29e-02  3.66e-02]\n","42467.52924853204\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mMXYYf3IKcnk","colab_type":"text"},"source":["Therefore, the equation of our multiple linear regression model is:\n","\n","$$\\textrm{Profit} = 86.6 \\times \\textrm{Dummy State 1} - 873 \\times \\textrm{Dummy State 2} + 786 \\times \\textrm{Dummy State 3} - 0.773 \\times \\textrm{R&D Spend} + 0.0329 \\times \\textrm{Administration} + 0.0366 \\times \\textrm{Marketing Spend} + 42467.53$$\n","\n","**Important Note:** To get these coefficients we called the \"coef_\" and \"intercept_\" attributes from our regressor object. Attributes in Python are different than methods and usually return a simple value or an array of values."]},{"cell_type":"markdown","metadata":{"id":"QfRK3Yhlpo3W","colab_type":"text"},"source":["predicted profit,actual profit is closer.to apply backward corelation manually to build model,check below code."]},{"cell_type":"code","metadata":{"id":"K-uuTdGxpXbO","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594299506378,"user_tz":-330,"elapsed":965,"user":{"displayName":"Chintu Chintu","photoUrl":"","userId":"16549076222200533733"}}},"source":["#Manually applying Backwar corelation\n","# # Multiple Linear Regression\n"," \n","# # Importing the libraries\n","# import numpy as np\n","# import matplotlib.pyplot as plt\n","# import pandas as pd\n"," \n","# # Importing the dataset\n","# dataset = pd.read_csv('50_Startups.csv')\n","# X = dataset.iloc[:, :-1].values\n","# y = dataset.iloc[:, -1].values\n","# print(X)\n"," \n","# # Encoding categorical data\n","# from sklearn.compose import ColumnTransformer\n","# from sklearn.preprocessing import OneHotEncoder\n","# ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [3])], remainder='passthrough')\n","# X = np.array(ct.fit_transform(X))\n","# print(X)\n"," \n","# # Avoiding the Dummy Variable Trap\n","# X = X[:, 1:]\n"," \n","# # Splitting the dataset into the Training set and Test set\n","# from sklearn.model_selection import train_test_split\n","# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n"," \n","# # Training the Multiple Linear Regression model on the Training set\n","# from sklearn.linear_model import LinearRegression\n","# regressor = LinearRegression()\n","# regressor.fit(X_train, y_train)\n"," \n","# # Predicting the Test set results\n","# y_pred = regressor.predict(X_test)\n","# np.set_printoptions(precision=2)\n","# print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))\n"," \n","# # Building the optimal model using Backward Elimination\n","# import statsmodels.api as sm\n","# X = np.append(arr = np.ones((50, 1)).astype(int), values = X, axis = 1)\n","# X_opt = X[:, [0, 1, 2, 3, 4, 5]]\n","# X_opt = X_opt.astype(np.float64)\n","# regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n","# regressor_OLS.summary()X_opt = X[:, [0, 1, 3, 4, 5]]\n","# X_opt = X_opt.astype(np.float64)\n","# regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n","# regressor_OLS.summary()X_opt = X[:, [0, 3, 4, 5]]\n","# X_opt = X_opt.astype(np.float64)\n","# regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n","# regressor_OLS.summary()X_opt = X[:, [0, 3, 5]]\n","# X_opt = X_opt.astype(np.float64)\n","# regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n","# regressor_OLS.summary()X_opt = X[:, [0, 3]]\n","# X_opt = X_opt.astype(np.float64)regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n","# regressor_OLS.summary()"],"execution_count":44,"outputs":[]},{"cell_type":"code","metadata":{"id":"C5SVCvg7pj7M","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}