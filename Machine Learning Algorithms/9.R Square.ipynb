{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"9.R Square.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOKNLLIi6r/wYKL5KaZxquE"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"fq7PfQHoEIa_","colab_type":"text"},"source":["# **R Square**\n","in simple linear regression model the line with the smallest SSres=sum(y1-yi^)^2 is the best simple linear regression model.\n","<img src=\"https://github.com/ProgramSKAN/Sample-Data-For-Machine-Learning/blob/master/rsquare1.PNG?raw=true\">\n","\n","instead of regression line , now draw a average line SStot=sum(y1-yavg^)^2.now R^2=1-(SSres/SStot).R^2 tells how good is your line compared to any line ex:avg line SStot.\n","\n","if regression line goes through all points then SSresidual=0 the R^2=1, Ideal senario.The closer R^2=1 the better.R^2 can be negative when SSres fits worse than SSavg line, which is rare.\n","<img src=\"https://github.com/ProgramSKAN/Sample-Data-For-Machine-Learning/blob/master/rsquare2.PNG?raw=true\">\n","\n","# **Adjusted R Square**\n","\n","even if we add unrelated feature x3 the coefficient b3 can't be 0 and R^2 never decrese.so we add a penalise factor.\n","\n","<img src=\"https://github.com/ProgramSKAN/Sample-Data-For-Machine-Learning/blob/master/rsquare3.PNG?raw=true\">\n","\n","<img src=\"https://github.com/ProgramSKAN/Sample-Data-For-Machine-Learning/blob/master/rsquare4.PNG?raw=true\">"]},{"cell_type":"code","metadata":{"id":"V_0Y98bzJv7D","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}